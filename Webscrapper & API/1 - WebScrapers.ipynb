{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ec38b6",
   "metadata": {},
   "source": [
    "# Webscrapping Macroeconomic Data and news event on Forex Factory\n",
    "### Import the necessary libraries\n",
    "\n",
    "- Pandas to create a data frame and store data\n",
    "- Time to refresh the page\n",
    "- Request to make request about a particular website\n",
    "- BeautifulSoup to parse the document using ‘lxml’\n",
    "- WebDriver to perform action on Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d64d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b9079",
   "metadata": {},
   "source": [
    "# Part 1: \n",
    "## Retrieving macroeconmoic indicators from Trading Economics\n",
    "\n",
    "- The code below works on most of Trading Economics Indicators, user can simply add any additional macroeconomic data by inputting an additional key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ee79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLS\n",
    "keys = [\"gdp-growth-rate\", \"inflation-rate\", \"interest-rate\",\n",
    "        \"unemployment-rate\", \"government-debt-to-gdp\", \n",
    "        \"balance-of-trade\", \"current-account-to-gdp\"]\n",
    "\n",
    "def get_all_url():\n",
    "     \n",
    "    list_of_url = []\n",
    "    \n",
    "    # URLs follow the same format according to the indicator\n",
    "    \n",
    "    for i in range(0, len(keys)):\n",
    "        url = f\"https://tradingeconomics.com/country-list/{keys[i]}?continent=world\"\n",
    "        list_of_url.append(url)\n",
    "        \n",
    "    return list_of_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e326dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_frame():\n",
    "    \n",
    "    \n",
    "    list_of_url = get_all_url()\n",
    "    \n",
    "    soups = []\n",
    "    \n",
    "    # Adding this header to avoid getting blocked by the website\n",
    "    \n",
    "    headers = {'User-Agent': \n",
    "                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "            \n",
    "    for i in range(len(list_of_url)):\n",
    "        \n",
    "        # Gather all useful HTML\n",
    "        \n",
    "        r = requests.get(list_of_url[i], headers=headers)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        soups.append(soup)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for soup in soups:\n",
    "\n",
    "        # Looking for the data tables for each URL\n",
    "        \n",
    "        div = soup.find('table', class_='table table-hover table-heatmap')\n",
    "        th = div.find_all('th')\n",
    "\n",
    "        columns_names = []\n",
    "\n",
    "        for item in th:\n",
    "            columns_names.append(item.text.strip())\n",
    "\n",
    "        rows_list = []\n",
    "\n",
    "        rows = soup.find_all(\"tr\", class_=\"datatable-row\")\n",
    "\n",
    "        for row in rows:\n",
    "\n",
    "            row_object = []\n",
    "\n",
    "            for item in row.find_all(\"td\"):\n",
    "\n",
    "                row_object.append(item.text.strip())\n",
    "\n",
    "            rows_list.append(row_object)   \n",
    "\n",
    "        #Converting to dataframe and append to the list of dataframes\n",
    "        exec(\"df_\"+str(count)+\" = pd.DataFrame(rows_list, columns =columns_names)\")\n",
    "        exec(\"df_list.append(\"+\"df_\"+str(count)+\")\")\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aded952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_csv():\n",
    "    \n",
    "    data_frame = get_data_frame()\n",
    "\n",
    "    # We now we'll loop the list of dataframes\n",
    "    for i, df in enumerate(data_frame):\n",
    "        df.to_csv(f\"CSV/{keys[i]}.csv\", index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "# Save CSV \n",
    "\n",
    "get_data_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f288229",
   "metadata": {},
   "source": [
    "# Part 2: \n",
    "\n",
    "## Retrieving news (figure announcments) from forexfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2846ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_frame_news():\n",
    "  \n",
    "  # Focusing on the current month\n",
    "    \n",
    "  month = \"Nov\" # change the month to last to make it run automatically on the last month\n",
    "  year = \"2022\" # change year if desired\n",
    "    \n",
    "  url = f'https://www.forexfactory.com/calendar?month={month}.{year}'\n",
    "\n",
    "\n",
    "  # we use sleep of 3 second to make sure data is fully loaded\n",
    "    \n",
    "  driver = webdriver.Firefox() #change to other navigator if needed\n",
    "  driver.get(url)\n",
    "  time.sleep(3)\n",
    "  html = driver.page_source\n",
    "  soup = BeautifulSoup(html,'html.parser')\n",
    "  driver.quit()\n",
    "\n",
    "\n",
    "  table_news = soup.find('table', class_='calendar__table')\n",
    "  list_of_rows = []\n",
    "\n",
    "  #Filtering events that have a href link\n",
    "\n",
    "  for row in table_news.find_all('tr', {'data-eventid':True}):\n",
    "      list_of_cells = []\n",
    "      \n",
    "      #Filtering high-impact events\n",
    "      \n",
    "      for span in row.find_all('span', class_='high'):\n",
    "          \n",
    "          #Extracting the values from the table data in each table row\n",
    "          \n",
    "          for cell in row.find_all('td', class_=[\n",
    "            'calendar__cell calendar__currency currency',\n",
    "            'calendar__cell calendar__date date',  \n",
    "            'calendar__cell calendar__time time',  \n",
    "            'calendar__cell calendar__event event', \n",
    "            'calendar__cell calendar__actual actual', \n",
    "            'calendar__cell calendar__forecast forecast', \n",
    "            'calendar__cell calendar__previous previous']):\n",
    "              \n",
    "              list_of_cells.append(cell.text)\n",
    "          list_of_rows.append(list_of_cells)\n",
    "\n",
    "      for span in row.find_all('span', class_='low'):\n",
    "          \n",
    "          #Extracting the values from the table data in each table row\n",
    "          \n",
    "          for cell in row.find_all('td', class_=[\n",
    "            'calendar__cell calendar__currency currency',\n",
    "            'calendar__cell calendar__date date',  \n",
    "            'calendar__cell calendar__time time',  \n",
    "            'calendar__cell calendar__event event', \n",
    "            'calendar__cell calendar__actual actual', \n",
    "            'calendar__cell calendar__forecast forecast', \n",
    "            'calendar__cell calendar__previous previous']):\n",
    "              \n",
    "              list_of_cells.append(cell.text)\n",
    "          list_of_rows.append(list_of_cells)\n",
    "\n",
    "  df_news = pd.DataFrame(list_of_rows, columns =\n",
    "  ['Date', 'Time','Currency', 'Event_Title', 'Actual', 'Forecast', 'Previous'])\n",
    "\n",
    "\n",
    "\n",
    "  # clean the data\n",
    "\n",
    "  df_news.Date = df_news.Date.str.split(' ').str[2] + \" \" + df_news.Date.str.split(' ').str[1].str[-3:]\n",
    "  df_news.Date = df_news.Date.fillna(method='ffill')\n",
    "\n",
    "  df_news.Time = df_news.Time.str.split('\\n').str[1].str[:-2] + ' ' + df_news.Time.str.split('\\n').str[1].str[-2:]\n",
    "  df_news.Time.fillna('', inplace=True)\n",
    "\n",
    "  df_news.Currency = df_news.Currency.str.split('\\n').str[1]\n",
    "  df_news.Date = df_news.Date.fillna(\"1 \" + month)\n",
    "  df_news = df_news\n",
    "\n",
    "  return df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a92827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this Cell to Save News as CSV file \n",
    "\n",
    "def get_data_news_csv():\n",
    "    data_frame = get_data_frame_news()\n",
    "    data_frame.to_csv(\"CSV/News.csv\", index=False)\n",
    "\n",
    "get_data_news_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
